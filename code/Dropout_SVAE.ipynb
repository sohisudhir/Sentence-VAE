{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dropout_SVAE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "muibpWFmDKC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "import torch.distributions as distb\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from datetime import datetime\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pprint import pprint\n",
        "from torch.utils.data import Dataset\n",
        "from collections import Counter\n",
        "from torch.distributions.normal import Normal\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87bj8h8aWavL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSxEIP4K6WE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pprint import pprint\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "class BaseTreeData(Dataset):\n",
        "    def __init__(self, train_data_path=\"02-21.10way.clean\", rare_threshold=1):\n",
        "        self.rare_threshold = rare_threshold\n",
        "        self.base_data = self.remove_tags(open(train_data_path, \"r\").read())\n",
        "        self.SOS = \"|new\"\n",
        "        self.EOS = \".\"\n",
        "        self.pad = \"<_>\"\n",
        "        self.rare = \"RARE\"\n",
        "        self.base_data.extend([self.SOS for i in range(self.rare_threshold + 1)])\n",
        "        self.base_data.extend([self.pad for i in range(self.rare_threshold + 1)])\n",
        "        self.base_data.extend([self.rare for i in range(self.rare_threshold + 1)])\n",
        "\n",
        "        self.vocab = list(set(self.base_data))        \n",
        "        self.define_mappings()\n",
        "\n",
        "        # self.sentences = [sentence for sentence in self.read_sentence(self.base_data)]\n",
        "        self.max_sentence_len = self.largest_sentence()\n",
        "\n",
        "    def define_mappings(self):\n",
        "        self.word_counter = Counter(self.base_data)\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        idx = 0\n",
        "        for w in self.vocab:\n",
        "            if self.word_counter[w] > self.rare_threshold:\n",
        "                self.word2idx[w] = idx\n",
        "                self.idx2word[idx] = w\n",
        "                idx+=1\n",
        "        self.vocab_size = len(self.word2idx)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        # sentence = self.sentences[idx]\n",
        "        # inputs =  [self.word2idx[w] if self.word_counter[w] > self.rare_threshold else self.word2idx[self.rare] for w in sentence]\n",
        "        # targets = [self.word2idx[w] if self.word_counter[w] > self.rare_threshold else self.word2idx[self.rare] for w in sentence[1:]]\n",
        "        # # add padding\n",
        "        # inputs.extend([self.word2idx[self.pad] for i in range(self.max_sentence_len - len(inputs))])\n",
        "        # targets.extend([self.word2idx[self.pad] for i in range(self.max_sentence_len - len(targets))])\n",
        "        # batch = (inputs, targets)\n",
        "        # return batch, len(sentence)\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def read_sentence(self, data):\n",
        "        sentence = [self.SOS]\n",
        "        for word in data:\n",
        "            sentence.append(word)\n",
        "            if word == self.EOS:\n",
        "                yield sentence\n",
        "                sentence = [self.SOS]\n",
        "\n",
        "    def remove_tags(self, data):\n",
        "        \"\"\"\n",
        "        Remove tree tags from Penn Treebank dataset\n",
        "        \"\"\"\n",
        "        data = data.split(\")\")\n",
        "        out = []\n",
        "        for e in data:\n",
        "            e = e.split(\" \")\n",
        "            if e[-1] != \"\":\n",
        "                out.append(e[-1])\n",
        "        return out\n",
        "\n",
        "    def largest_sentence(self):\n",
        "        max_len = 0\n",
        "        for sentence in self.read_sentence(self.base_data):\n",
        "            if len(sentence) > max_len:\n",
        "                max_len = len(sentence)\n",
        "        return max_len\n",
        "\n",
        "    def convert_to_string(self, char_idx):\n",
        "        return ' '.join(self.idx2word[idx] for idx in char_idx)\n",
        "\n",
        "    def convert_to_idx(self, chars):\n",
        "        return [self.word2idx[char] for char in chars]\n",
        "\n",
        "    def __len__(self):\n",
        "        # return len(self.sentences)\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class PennTreeData(BaseTreeData):\n",
        "    def __init__(self, data_path):\n",
        "        super().__init__()\n",
        "        self.data = self.remove_tags(open(data_path, \"r\").read())\n",
        "        self.sentences = [sentence for sentence in self.read_sentence(self.data)]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        inputs =  [self.word2idx[w] if self.word_counter[w] > self.rare_threshold else self.word2idx[self.rare] for w in sentence]\n",
        "        targets = [self.word2idx[w] if self.word_counter[w] > self.rare_threshold else self.word2idx[self.rare] for w in sentence[1:]]\n",
        "        # add padding\n",
        "        inputs.extend([self.word2idx[self.pad] for i in range(self.max_sentence_len - len(inputs))])\n",
        "        targets.extend([self.word2idx[self.pad] for i in range(self.max_sentence_len - len(targets))])\n",
        "        batch = (inputs, targets)\n",
        "        return batch, len(sentence)\n",
        "        \n",
        "    def __len__(self):\n",
        "      return len(self.sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_c2i_ESDQ8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class encoder(nn.Module):\n",
        "  \n",
        "  def __init__(self, vocabulary_size, emb_dim, hidden_dim, lstm_layers, latent_size, device = 'cuda:0', dropout = 0):\n",
        "    \n",
        "    super(encoder,self).__init__()\n",
        "    self.embedding = nn.Embedding(num_embeddings=vocabulary_size,embedding_dim=emb_dim)\n",
        "    self.lstm = nn.LSTM(input_size = emb_dim, hidden_size = hidden_dim, num_layers=lstm_layers, batch_first=False, bidirectional = True)\n",
        "    self.mean_encoding = nn.Linear(hidden_dim * 2, latent_size)\n",
        "    self.sd_encoding = nn.Linear(hidden_dim * 2, latent_size)\n",
        "    self.softplus = nn.Softplus()\n",
        "#     self.dropout = dropout                 #do later\n",
        "#     self.emb_dropout = nn.Dropout(p)\n",
        "    \n",
        "  def forward(self, x, hid = None):\n",
        "    \n",
        "    x = x.to(device)\n",
        "    x = self.embedding(x)\n",
        "    \n",
        "    _ , hidden = self.lstm(x, hid)\n",
        "    \n",
        "    h = torch.cat([hidden[0], hidden[1]], dim=-1)\n",
        "    \n",
        "#     hidden = torch.stack(hidden).view(hidden[0].size(0),hidden[0].size(1),-1)\n",
        "    \n",
        "    mean = self.mean_encoding(h)\n",
        "\n",
        "    sd = self.softplus(self.sd_encoding(h))\n",
        "    \n",
        "    logvar = None\n",
        "    \n",
        "    return mean, logvar, sd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3V99PoNIxWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#USING THE BASELINE WITH SOME MODIFICATIONS AS DECODER\n",
        "\n",
        "class RNNLM(nn.Module):\n",
        "    def __init__(self, vocabulary_size, bidirectional, embedding_dim,\n",
        "                 lstm_num_hidden=512, lstm_num_layers=3, device='cuda:0'):\n",
        "        super(RNNLM, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size,\n",
        "                                      embedding_dim=embedding_dim)\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                            hidden_size=lstm_num_hidden,\n",
        "                            num_layers=lstm_num_layers,\n",
        "                            batch_first=False, bidirectional = bidirectional)\n",
        "        \n",
        "        \n",
        "        self.linear = nn.Linear(lstm_num_hidden*2, vocabulary_size)\n",
        "        \n",
        "    def forward(self, x, hidden = None, flag = True):\n",
        "        \n",
        "#         x = self.embedding(x)\n",
        "        all_hidden, hidden = self.lstm(x, (hidden, hidden))\n",
        "        out = self.linear(all_hidden)\n",
        "\n",
        "        return out, hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMaqHHnZI3Qd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class senvae(nn.Module):\n",
        "  \n",
        "  def __init__(self, emb_dim, padding_index, vocab_size, hidden_dim, lstm_layers, latent_size,  word_dropout = 0.,epoch = 1, anneal = 'logistic',  device = 'cuda:0' ):\n",
        "    \n",
        "    super(senvae,self).__init__()\n",
        "    \n",
        "    self.encoder = encoder(vocab_size, emb_dim, hidden_dim , lstm_layers, latent_size, device = 'cuda:0', dropout = 0)\n",
        "    self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                      embedding_dim=emb_dim)\n",
        "    self.dropout = nn.Dropout(p = word_dropout)\n",
        "    self.anneal = anneal\n",
        "    self.latent_dim = latent_size\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.pad_idx = padding_index\n",
        "    self.emb_dim = emb_dim\n",
        "    self.vocab_size = vocab_size\n",
        "    self.lstm_layers = lstm_layers\n",
        "    self.epsilon = 1e-7\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.latenttohidden = nn.Linear(latent_size, hidden_dim)\n",
        "    self.criterion = nn.CrossEntropyLoss(ignore_index = padding_index)\n",
        "    self.nllcriterion = nn.CrossEntropyLoss(ignore_index = padding_index) #, reduction = 'None')\n",
        "    self.recons_loss = torch.zeros([2 * self.lstm_layers, 1])\n",
        "    self.decoder = RNNLM(vocab_size, bidirectional=True, embedding_dim=emb_dim,\n",
        "                 lstm_num_hidden=hidden_dim, lstm_num_layers=lstm_layers, device='cuda:0')  \n",
        "\n",
        "  def forward(self, x, targets, n = 10):\n",
        "    \n",
        "    x = x.to(device)\n",
        "       \n",
        "    mean, logvar, sd = self.encoder(x)\n",
        "    \n",
        "    kl_scale = self.kl_anneal_function('linear', self.epoch)\n",
        "    \n",
        "    kl_loss = (sd ** 2 + mean**2 - ((sd) ** 2 + self.epsilon).log() - 1).sum(1).sum(1) * 0.5\n",
        "  \n",
        "    kl_loss = kl_loss * kl_scale\n",
        "    \n",
        "    emb_x = self.embedding(x)\n",
        "    \n",
        "    emb_x = self.dropout(emb_x)\n",
        "    \n",
        "    #calculating loss for multisample ELBO\n",
        "    \n",
        "    r = torch.FloatTensor([0]).to(device)\n",
        "    \n",
        "    for i in range(n):\n",
        "    \n",
        "      z = torch.randn([x.size(1), self.latent_dim]).to(device)\n",
        "      \n",
        "      z = z * sd + mean\n",
        "\n",
        "      h = self.tanh(self.latenttohidden(z))\n",
        "\n",
        "      output, _ = self.decoder(emb_x, h)\n",
        "      output = output.view(x.size()[0]* x.size()[1], -1)\n",
        "\n",
        "      reconstruction_loss = F.cross_entropy(output, targets, ignore_index=self.pad_idx, \n",
        "                                                reduction=\"none\")\n",
        "      \n",
        "      r += reconstruction_loss.sum(dim=0)\n",
        " \n",
        "    rl = torch.div(r,n)  \n",
        "    total_loss = kl_loss.mean()  + rl\n",
        "    \n",
        "    h0 = self.tanh(self.latenttohidden(mean))\n",
        "    out, _ = self.decoder(emb_x, h0)\n",
        "    \n",
        "    nll = None\n",
        "    return out, total_loss, nll  \n",
        "  \n",
        "  def temperature_sample(self, h, temperature):\n",
        "    h = h.squeeze()\n",
        "    distribution = torch.softmax(h/temperature, dim=0)    \n",
        "    return torch.multinomial(distribution,1).item()\n",
        "  \n",
        "  def test(self,dataset, first_word, device, decoding = \"greedy\", flag = True, max_len = 20, temperature = 2.0):\n",
        "\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      x = torch.tensor(first_word, dtype=torch.long, device=device).view(1,1)\n",
        "      sentence = [x.item()]\n",
        "      mean, _ ,  sd = self.encoder(x)\n",
        "      eps = distb.MultivariateNormal(torch.zeros(mean.size(2)).to(device = 'cuda:0'), torch.eye(mean.size(2)).to(device = 'cuda:0'))\n",
        "      z = mean + eps.sample() * sd  \n",
        "      if(flag):\n",
        "        h0 = self.tanh(self.latenttohidden(mean)) \n",
        "      else:\n",
        "        h0 = self.tanh(self.latenttohidden(z)) \n",
        "      \n",
        "      # use this z for predicting all the words until '.' is found\n",
        "      all_hidden, hidden = self.decoder(x , h0)\n",
        "      \n",
        "      if(decoding == \"greedy\"):\n",
        "        sampled_word = torch.argmax(all_hidden, dim=2).item()\n",
        "      else:\n",
        "        sampled_word = self.temperature_sample(all_hidden, temperature)\n",
        "      sentence.append(sampled_word)\n",
        "      x = torch.tensor(sampled_word, dtype=torch.long, device=device).view(1,1)\n",
        "      \n",
        "      while(sentence[-1] != dataset.EOS and len(sentence) < max_len):\n",
        "        all_hidden, hidden = self.decoder(x, h0)\n",
        "        if(decoding == \"greedy\"):\n",
        "          sampled_word = torch.argmax(all_hidden, dim=2).item()\n",
        "        else:\n",
        "          sampled_word = self.temperature_sample(all_hidden, temperature)\n",
        "        sentence.append(sampled_word)\n",
        "        x = torch.tensor(sampled_word, dtype=torch.long, device=device).view(1,1)\n",
        "\n",
        "    return(sentence)\n",
        "\n",
        "  def test_interpolate(self,dataset, first_word, mean, sd, decoding = \"greedy\", flag = True, max_len = 20, temperature = 2.0):\n",
        "\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      x = torch.tensor(first_word, dtype=torch.long, device=device).view(1,1)\n",
        "      sentence = [x.item()]\n",
        "      eps = distb.MultivariateNormal(torch.zeros(mean.size(2)).to(device = 'cuda:0'), torch.eye(mean.size(2)).to(device = 'cuda:0'))\n",
        "      z = mean + eps.sample() * sd  \n",
        "      if(flag):\n",
        "        h0 = self.tanh(self.latenttohidden(mean)) \n",
        "      else:\n",
        "        h0 = self.tanh(self.latenttohidden(z)) \n",
        "      \n",
        "      # use this z for predicting all the words until '.' is found\n",
        "      emb_x = self.embedding(x)\n",
        "      all_hidden, hidden = self.decoder(emb_x , h0)\n",
        "      \n",
        "      if(decoding == \"greedy\"):\n",
        "        sampled_word = torch.argmax(all_hidden, dim=2).item()\n",
        "      else:\n",
        "        sampled_word = self.temperature_sample(all_hidden, temperature)\n",
        "      sentence.append(sampled_word)\n",
        "      x = torch.tensor(sampled_word, dtype=torch.long, device=device).view(1,1)\n",
        "      \n",
        "      while(sentence[-1] != dataset.EOS and len(sentence) < max_len):\n",
        "        all_hidden, hidden = self.decoder(emb_x, h0)\n",
        "        if(decoding == \"greedy\"):\n",
        "          sampled_word = torch.argmax(all_hidden, dim=2).item()\n",
        "        else:\n",
        "          sampled_word = self.temperature_sample(all_hidden, temperature)\n",
        "        sentence.append(sampled_word)\n",
        "        x = torch.tensor(sampled_word, dtype=torch.long, device=device).view(1,1)\n",
        "\n",
        "    return(sentence)\n",
        "\n",
        "  def perplexity(self, model, dataloader, n = 5):\n",
        "    \n",
        "    #calculate perlpexity for entire dataset\n",
        "    log_px = 0.\n",
        "    num_sen = 0\n",
        "    num_preds = 0\n",
        "    accuracies = []\n",
        "    with torch.no_grad():\n",
        "    \n",
        "      for step, (batch, sen_lens) in enumerate(dataloader):\n",
        "\n",
        "        batch_inputs, batch_targets = batch\n",
        "            \n",
        "        batch_inputs = torch.stack(batch_inputs).to(device)\n",
        "        batch_targets = torch.stack(batch_targets).to(device).view(-1)\n",
        "        \n",
        "        mean,logvar, sd = self.encoder(batch_inputs)\n",
        "        \n",
        "        likelihoods = torch.FloatTensor(n,2,100).to(device)\n",
        "        losses = []\n",
        "        for i in range(n):\n",
        "\n",
        "          z = torch.randn([batch_inputs.size(1), self.latent_dim]).to(device)\n",
        "          z = z * sd + mean\n",
        "          h = self.tanh(self.latenttohidden(z))\n",
        "      \n",
        "          emb_x = self.embedding(batch_inputs)\n",
        "          output, _ = self.decoder(emb_x, h)\n",
        "          output = output.view(batch_inputs.size()[0]* batch_inputs.size()[1], -1)\n",
        "          accuracy = (output.argmax(1) == batch_targets).float().sum()/(batch_targets != 0).sum()\n",
        "          accuracies.append(accuracy.item())\n",
        "          \n",
        "          #p(x|z)\n",
        "          logprob = F.cross_entropy(output, batch_targets, reduction=\"sum\")\n",
        "\n",
        "          # Calculate p(z|prior) and p(z|encoding) \n",
        "          \n",
        "          #prior_pz\n",
        "          prior = Normal(torch.zeros((mean.shape[1], self.latent_dim), device = device),torch.ones((mean.shape[1], self.latent_dim), device = device))\n",
        "          #posterior_qz\n",
        "          encoded_distribution = Normal(mean, sd)\n",
        "          normalizer = (prior.log_prob(z).exp()/encoded_distribution.log_prob(z).exp()).prod(1).unsqueeze(1)\n",
        "          \n",
        "          a = prior.log_prob(z).sum(dim = 1)\n",
        "          b = encoded_distribution.log_prob(z).sum(dim = 1)\n",
        "          c = logprob + a - b\n",
        " \n",
        "          d = logprob*normalizer +(logprob == 0).float()\n",
        "\n",
        "          likelihoods[i] = c\n",
        "        \n",
        "\n",
        "        log_px_batch = torch.logsumexp(likelihoods, dim = 0) - torch.log(torch.Tensor([n]).to(device))\n",
        "        log_px += log_px_batch.sum(-1)\n",
        "  \n",
        "        num_sen += sen_lens.size(0)\n",
        "        num_preds += sen_lens.sum()\n",
        "    \n",
        "    perplexity = log_px / (num_preds * batch_inputs.size(1))\n",
        "    NLL = log_px / num_sen\n",
        "    \n",
        "    return perplexity.mean(), NLL.mean(), np.mean(accuracies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VZTYSJMQNdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ELBO(model, dataloader, n = 10):\n",
        "  \n",
        "#   model.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    \n",
        "    rec_loss = torch.FloatTensor([0]).to(device)\n",
        "    kld = torch.FloatTensor([0]).to(device)\n",
        "    num_sen = 0\n",
        "    for step, (batch, sen_lens) in enumerate(dataloader):\n",
        "      \n",
        "      batch_inputs, batch_targets = batch      \n",
        "      batch_inputs = torch.stack(batch_inputs).to(device)\n",
        "      batch_targets = torch.stack(batch_targets).to(device).view(-1)\n",
        "      emb_x = model.embedding(batch_inputs)\n",
        "      \n",
        "      mean, _ , sd = model.encoder(batch_inputs)\n",
        "      \n",
        "\n",
        "      kl_loss = (sd ** 2 + mean**2 - ((sd ** 2) + model.epsilon).log() - 1).sum(1).sum(1) * 0.5\n",
        "\n",
        "      \n",
        "      #calculating loss for multisample ELBO\n",
        "\n",
        "      r = torch.FloatTensor([0]).to(device)\n",
        "      for i in range(n):\n",
        "\n",
        "        z = torch.randn([batch_inputs.size(1), model.latent_dim]).to(device)\n",
        "\n",
        "        z = z * sd + mean\n",
        "\n",
        "        h = model.tanh(model.latenttohidden(z))\n",
        "\n",
        "        output, _ = model.decoder(emb_x, h)\n",
        "        output = output.view(batch_inputs.size()[0]* batch_inputs.size()[1], -1)\n",
        "\n",
        "        reconstruction_loss = F.cross_entropy(output, batch_targets, ignore_index= model.pad_idx, \n",
        "                                                  reduction=\"none\")\n",
        "\n",
        "        r += reconstruction_loss.sum(dim=0)\n",
        "\n",
        "      rec_loss += r\n",
        "#       rec_loss = torch.div(rec_loss, n)\n",
        "      kld += kl_loss.mean()\n",
        "      num_sen += batch_inputs.size(1) #batch size\n",
        "      \n",
        "    #Average values\n",
        "    rec_loss = torch.div(rec_loss,num_sen)\n",
        "    normalizer = num_sen * (step + 1)\n",
        "    kld1 = torch.div(kld, normalizer)\n",
        "    kld2 = torch.div(kld, num_sen)\n",
        "    elbo = rec_loss + kld1\n",
        "    \n",
        "  return elbo.item() , kld1.item()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtTKKMH1x4hZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def interpolation(test_dataset, test_data_loader, idx1, idx2, model):\n",
        "  \n",
        "  with torch.no_grad():\n",
        "\n",
        "    for i,j in test_data_loader:\n",
        "      s1 = []\n",
        "      s2 = []\n",
        "      for k in range(16):\n",
        "        #inside a [batch])\n",
        "        for a in i[k]:\n",
        "          #inside one batch element\n",
        "          b = a[idx1]\n",
        "          c = a[idx2]\n",
        "          s1.append(b)\n",
        "          s2.append(c)\n",
        "        break\n",
        "      break\n",
        "    a = torch.zeros(140,1).to(device)\n",
        "    d = torch.zeros(140,1).to(device)\n",
        "    for i in range(140):\n",
        "      a[i] = s1[i]\n",
        "      d[i] = s2[i]\n",
        "\n",
        "    print(\"First original statement\")\n",
        "    for i in a:\n",
        "      print(test_dataset.idx2word[i.item()], end = \" \")\n",
        "    print(\"\\n2nd original statement\")\n",
        "    for i in d:\n",
        "      print(test_dataset.idx2word[i.item()], end = \" \")\n",
        "    print(\"\")\n",
        "    s1 = a\n",
        "    s1 = s1.long().to(device)\n",
        "    s2 = d\n",
        "    s2 = s2.long().to(device)\n",
        "\n",
        "    mean1, _, sd1 = model.encoder(s1)\n",
        "    mean2, _, sd2 = model.encoder(s2)\n",
        "\n",
        "    mean_diff = (mean2 - mean1)/(idx2-idx1 +1)\n",
        "    means = [mean1 + mean_diff*i for i in range(10)]\n",
        "\n",
        "    for i, mean in enumerate(means):\n",
        "      generated_sample =  model.test_interpolate(test_dataset, test_dataset.word2idx[test_dataset.SOS], mean, sd1, decoding = \"greedy\", flag = True)\n",
        "      print(\"Interpolated reconstruction: \",test_dataset.convert_to_string(generated_sample))\n",
        "\n",
        "def reconstruct(model, test_dataset, test_data_loader, idx):\n",
        "  \n",
        "  with torch.no_grad():\n",
        "\n",
        "    for i,j in test_data_loader:\n",
        "      s1 = []\n",
        "      for k in range(16):\n",
        "        #inside a [batch])\n",
        "        for a in i[k]:\n",
        "          #inside one batch element\n",
        "          b = a[idx]\n",
        "          s1.append(b)\n",
        "        break\n",
        "      break\n",
        "    a = torch.zeros(140,1).to(device)\n",
        "    for i in range(140):\n",
        "      a[i] = s1[i]\n",
        "\n",
        "    print(\"Original statement\")\n",
        "    for i in a:\n",
        "      print(test_dataset.idx2word[i.item()], end = \" \")\n",
        "    print(\"\")\n",
        "    s1 = a\n",
        "    s1 = s1.long().to(device)\n",
        "    mean, _, sd = model.encoder(s1)\n",
        "\n",
        "    #flag = true for mean\n",
        "    generated_sample = model.test_interpolate(test_dataset, test_dataset.word2idx[test_dataset.SOS],mean, sd, decoding = \"greedy\", flag = True)\n",
        "    print(\"SAMPLE (gmean)\", \":\", test_dataset.convert_to_string(generated_sample))\n",
        "    generated_sample = model.test_interpolate(test_dataset, test_dataset.word2idx[test_dataset.SOS],mean, sd, decoding = \"temp\", flag = True)\n",
        "    print(\"SAMPLE (tmean)\", \":\", test_dataset.convert_to_string(generated_sample))\n",
        "\n",
        "    #flag = false for z\n",
        "    for i in range(10):\n",
        "      generated_sample = model.test_interpolate(test_dataset, test_dataset.word2idx[test_dataset.SOS],mean, sd, decoding = \"greedy\", flag = False)\n",
        "      print(\"SAMPLE_g\", i+1, \":\", test_dataset.convert_to_string(generated_sample))\n",
        "      generated_sample = model.test_interpolate(test_dataset, test_dataset.word2idx[test_dataset.SOS],mean, sd, decoding = \"temp\", flag = False)\n",
        "      print(\"SAMPLE_t\", i+1, \":\", test_dataset.convert_to_string(generated_sample))\n",
        "    \n",
        "def reconstruct_val(model, test_dataset, test_data_loader, idx):\n",
        "  \n",
        "  with torch.no_grad():\n",
        "\n",
        "    for i,j in test_data_loader:\n",
        "      s1 = []\n",
        "      for k in range(16):\n",
        "        #inside a [batch])\n",
        "        for a in i[k]:\n",
        "          #inside one batch element\n",
        "          b = a[idx]\n",
        "          s1.append(b)\n",
        "        break\n",
        "      break\n",
        "    a = torch.zeros(140,1).to(device)\n",
        "    for i in range(140):\n",
        "      a[i] = s1[i]\n",
        "\n",
        "    print(\"Original statement\")\n",
        "    for i in a:\n",
        "      print(test_dataset.idx2word[i.item()], end = \" \")\n",
        "    print(\"\")\n",
        "    s1 = a\n",
        "    s1 = s1.long().to(device)\n",
        "    mean, _, sd = model.encoder(s1)\n",
        "\n",
        "    #flag = true for mean\n",
        "    generated_sample = model.test_interpolate(test_dataset, test_dataset.word2idx[test_dataset.SOS],mean, sd, decoding = \"greedy\", flag = True)\n",
        "    print(\"SAMPLE (gmean)\", \":\", test_dataset.convert_to_string(generated_sample))\n",
        "    generated_sample = model.test_interpolate(test_dataset, test_dataset.word2idx[test_dataset.SOS],mean, sd, decoding = \"temp\", flag = True)\n",
        "    print(\"SAMPLE (tmean)\", \":\", test_dataset.convert_to_string(generated_sample))\n",
        "\n",
        "    #flag = false for z\n",
        "    for i in range(1):\n",
        "      generated_sample = model.test_interpolate(test_dataset, test_dataset.word2idx[test_dataset.SOS],mean, sd, decoding = \"greedy\", flag = False)\n",
        "      print(\"SAMPLE (gz)\", \":\", test_dataset.convert_to_string(generated_sample))\n",
        "      generated_sample = model.test_interpolate(test_dataset, test_dataset.word2idx[test_dataset.SOS],mean, sd, decoding = \"temp\", flag = False)\n",
        "      print(\"SAMPLE (tz)\", \":\", test_dataset.convert_to_string(generated_sample))\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCM0ACbVtcpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_TEXT_FILE = \"02-21.10way.clean\"\n",
        "EVAL_TEXT_FILE = \"22.auto.clean\"\n",
        "TEST_TEXT_FILE = \"23.auto.clean\"\n",
        "  \n",
        "def train():\n",
        "  \n",
        "    \n",
        "    batch_size = 32\n",
        "    embedding_dimension = 128\n",
        "    lstm_num_hidden_l = 128\n",
        "    lstm_num_layers_l = 1\n",
        "    learning_rate = 0.001\n",
        "    n_epochs = 10\n",
        "    sample_method = 'greedy'\n",
        "    latent_size = 100\n",
        "    \n",
        "    # Initialize the dataset and data loader (note the +1)\n",
        "    train_dataset = PennTreeData(TRAIN_TEXT_FILE)\n",
        "    train_data_loader = DataLoader(train_dataset, batch_size, num_workers=1)\n",
        "    eval_dataset = PennTreeData(EVAL_TEXT_FILE)\n",
        "    eval_data_loader = DataLoader(eval_dataset, batch_size, num_workers=1)\n",
        "    test_dataset = PennTreeData(TEST_TEXT_FILE)\n",
        "    test_data_loader = DataLoader(test_dataset, batch_size, num_workers=1)\n",
        "    \n",
        "    # Initialize the model that we are going to use\n",
        "    model = senvae (embedding_dimension, train_dataset.word2idx[train_dataset.pad],  \n",
        "                    train_dataset.vocab_size, lstm_num_hidden_l, lstm_num_layers_l,\n",
        "                    latent_size, word_dropout = 0.25,epoch = 1, device = 'cuda:0')\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    # Setup the loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index = train_dataset.word2idx[train_dataset.pad])\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr = learning_rate)\n",
        "    \n",
        "    train_elbo = []\n",
        "    train_accuracies = []\n",
        "    val_elbo = []\n",
        "    val_accuracies = []\n",
        "    val_kld = []\n",
        "    val_ppl = []\n",
        "    val_nll = []\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"In epoch\", epoch)\n",
        "        for step, (batch, sen_lens) in enumerate(train_data_loader):\n",
        "            \n",
        "            model.zero_grad()\n",
        "            batch_inputs, batch_targets = batch\n",
        "            \n",
        "            # Only for time measurement of step through network\n",
        "            t1 = time.time()\n",
        "            batch_inputs = torch.stack(batch_inputs).to(device)\n",
        "            batch_targets = torch.stack(batch_targets).to(device).view(-1)\n",
        "\n",
        "  \n",
        "            output, loss, nll = model(batch_inputs, batch_targets)\n",
        "\n",
        "            output = output.view(batch_inputs.size()[0] * batch_inputs.size()[1], -1)\n",
        "            tloss = criterion(output,batch_targets)\n",
        "            accuracy = (output.argmax(1) == batch_targets).float().sum()/(batch_targets != 0).sum()\n",
        "            elbo = -loss            \n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            # For plotting\n",
        "            train_accuracies.append(accuracy)\n",
        "            train_elbo.append(-loss.item())\n",
        "            \n",
        "            # Just for time measurement\n",
        "            t2 = time.time()\n",
        "            examples_per_second = batch_size/float(t2-t1)\n",
        "           \n",
        "            if step % 100 == 0 : #config.print_every == 0:\n",
        "                print(\"[{}] Epoch {} Train Step {:00f}, Batch Size = {}, Examples/Sec = {:.2f}, \"\n",
        "                    \"Accuracy = {:.3f}, Loss = {:.5f} \".format(\n",
        "                        datetime.now().strftime(\"%Y-%m-%d %H:%M\"), epoch, step,\n",
        "                        batch_size, examples_per_second,\n",
        "                        accuracy, loss.item()))\n",
        "           \n",
        "          \n",
        "                print(\"Epoch\", epoch, \" completed\")\n",
        "                model.eval()\n",
        "                ppl, nll, acc = model.perplexity(model, eval_data_loader, n =10)\n",
        "                elbo, kld = ELBO(model, eval_data_loader)\n",
        "                val_accuracies.append(acc)\n",
        "                val_elbo.append(-elbo)\n",
        "                val_kld.append(kld)\n",
        "                val_ppl.append(ppl)\n",
        "                val_nll.append(nll)\n",
        "                print(\"Validation metrics: Batch Size = {}, Accuracy = {}, KL-D = {}, ELBO = {},\" \n",
        "                      \"Perplexity = {}, NLL = {} \".format(\n",
        "                          batch_size, acc, kld, elbo, ppl.item(), nll.item()))\n",
        "                generated_sample = reconstruct_val(model, eval_dataset, eval_data_loader, 1)\n",
        "#                 model.train()\n",
        "                break\n",
        "        break\n",
        "\n",
        "    print('Done training! Evaluating test set now')\n",
        "    model.eval()\n",
        "    ppl, nll, acc = model.perplexity(model, test_data_loader, n =10)\n",
        "    elbo, kld = ELBO(model, test_data_loader)\n",
        "    print(\"Test metrics: Batch Size = {}, Accuracy = {}, KL-D = {}, ELBO = {},\" \n",
        "          \"Perplexity = {}, NLL = {} \".format(\n",
        "              batch_size, acc, kld, elbo, ppl.item(), nll.item()))\n",
        "    print(\"**************RECONSTRUCTION CAPABILITY***************************\")\n",
        "    reconstruct(model, test_dataset, test_data_loader, 1)\n",
        "    print(\"****************INTERPOLATION CAPABILITY**************************\")\n",
        "    interpolation(test_dataset, test_data_loader, 0, 9, model)\n",
        "      \n",
        "    plt.plot(train_elbo, label = \"Train Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"train_elbo.png\")\n",
        "    plt.close()\n",
        "    \n",
        "    plt.plot(train_accuracies, label = \"Train accuracy\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"train_acc.png\")\n",
        "    plt.close()\n",
        "    \n",
        "    plt.plot(val_elbo, label = \"Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"val_elbo.png\")\n",
        "    plt.close()\n",
        "    \n",
        "    plt.plot(val_accuracies, label = \"Val accuracy\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"val_acc.png\")\n",
        "    plt.close()\n",
        "    \n",
        "    plt.plot(val_kld, label = \"val kld\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"val_kld.png\")\n",
        "    plt.close()\n",
        "    \n",
        "    return model, train_elbo, train_accuracies, val_elbo, val_accuracies, val_kld , val_ppl, val_nll\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqJ8csMcGEqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model, train_elbo, train_accuracies, val_elbo, val_accuracies, val_kld, val_ppl, val_nll  = train()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8rp6qJnjJPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.save(\"train_elbo\", train_elbo)\n",
        "# np.save(\"train_acc\", train_accuracies)\n",
        "# np.save(\"val_elbo\", val_elbo)\n",
        "# np.save(\"val_accuracies\", val_accuracies)\n",
        "# np.save(\"val_kld\", val_kld)\n",
        "# np.save(\"val_ppl\", val_ppl)\n",
        "# np.save(\"val_nll\", val_nll)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}